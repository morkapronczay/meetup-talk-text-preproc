{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia as wp\n",
    "import requests\n",
    "import bs4\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "\n",
    "with open('languages.txt', 'r') as file:\n",
    "    languages = [f.strip('\\n') for f in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIND 100 MOST INFLUENTIAL PEOPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url of 100 most influential people in history\n",
    "url = 'https://www.biographyonline.net/people/100-most-influential.html'\n",
    "r = requests.get(url)\n",
    "soup = bs4.BeautifulSoup(r.content, 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_elements = soup.find_all('li', class_ = None)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_from_element(tag):\n",
    "    try:\n",
    "        return tag.find_all('a')[0].text.strip()\n",
    "    except:\n",
    "        ret = tag.text.split('(')[0].strip()\n",
    "        if 'Menes' in ret:\n",
    "            return ret.split(' ')[0]\n",
    "        else:\n",
    "            return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [get_name_from_element(elem) for elem in name_elements]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT CONTENT OF THEIR WIKIPEDIA PAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kapronczaym/.local/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /usr/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "de\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "hu\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "ro\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "# create dir if not exists\n",
    "if not os.path.exists('page_text'):\n",
    "    os.makedirs('page_text') \n",
    "\n",
    "# download pages in every languages\n",
    "for lang in languages:\n",
    "    print(lang)\n",
    "    wp.set_lang(lang)\n",
    "    files[lang] = {}\n",
    "    for i, name in enumerate(names):\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "        try:\n",
    "            page_content = wp.page(wp.search(name)[0]).content\n",
    "            files[lang][name] = page_content\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIND 100 BEST CITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url of 100 best cities to live list\n",
    "url = 'https://www.bestcities.org/rankings/worlds-best-cities/'\n",
    "r = requests.get(url)\n",
    "soup = bs4.BeautifulSoup(r.content, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [' '.join(match.text.strip().split(' ')[1:]) for match in soup.findAll('div', {'class': 'rankings-cities-detail'})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIND THEIR WIKIPEDIA PAGES\n",
    "\n",
    "#### EXTRACT CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "de\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "hu\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "ro\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "# download pages in every language\n",
    "for lang in languages:\n",
    "    print(lang)\n",
    "    wp.set_lang(lang)\n",
    "    for i, name in enumerate(cities):\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "        try:\n",
    "            page_content = wp.page(wp.search(name)[0]).content\n",
    "            files[lang][name] = page_content\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIND TOP 100 COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url of top 100 companies list\n",
    "url = 'https://ceoworld.biz/2019/06/28/the-top-100-best-performing-companies-in-the-world-2019/'\n",
    "r = requests.get(url, headers=headers)\n",
    "soup = bs4.BeautifulSoup(r.content, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = [f.text for f in soup.findAll('td', {'class': 'column-2'})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIND THEIR WIKIPEDIA PAGES\n",
    "\n",
    "#### EXTRACT CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "de\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "hu\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "ro\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "# download pages in every language\n",
    "for lang in languages:\n",
    "    print(lang)\n",
    "    wp.set_lang(lang)\n",
    "    for i, name in enumerate(companies):\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "        try:\n",
    "            page_content = wp.page(wp.search(name)[0]).content\n",
    "            files[lang][name] = page_content\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIND TOP100 POP/ROCK BANDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url of top 100 companies list\n",
    "url = 'https://www.imdb.com/list/ls076954447/'\n",
    "r = requests.get(url, headers=headers)\n",
    "soup = bs4.BeautifulSoup(r.content, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [f.text.strip().split('\\n')[-1].strip() for f in soup.findAll('h3', {'class': 'lister-item-header'})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIND THEIR WIKIPEDIA PAGES\n",
    "\n",
    "#### EXTRACT CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "de\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "hu\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "ro\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "# download pages in every language\n",
    "for lang in languages:\n",
    "    print(lang)\n",
    "    wp.set_lang(lang)\n",
    "    for i, name in enumerate(bands):\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "        try:\n",
    "            page_content = wp.page(wp.search(name)[0]).content\n",
    "            files[lang][name] = page_content\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KEEP ONLY PAGES WHICH ARE PRESENT IN ALL LANGUAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, key in enumerate(files.keys()):\n",
    "    if i == 0:\n",
    "        common_files = set(files[key])\n",
    "    else:\n",
    "        common_files = common_files.intersection(set(files[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WRITE TO FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    if not os.path.exists(os.path.join('page_text', f'{lang}')):\n",
    "        os.makedirs(os.path.join('page_text', f'{lang}'))\n",
    "    for doc in common_files:\n",
    "        with open((os.path.join('page_text', f'{lang}', f'{doc.lower().replace(\" \", \"-\").replace(\"/\", \"\")}.txt')), 'w') as file:\n",
    "            file.write(files[lang][doc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
